{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tic_tac_toe.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPmiPm37M22QB7G3BoyLKHp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x24w0pqG9PDR"},"source":["## TIC TAC TOE\r\n"]},{"cell_type":"markdown","metadata":{"id":"E5VGq9nF9fFU"},"source":["This project is based upon Reinforcement Learning which is a technique in which most of the bots and AI agent learns to do things.\r\n","\r\n","In this project i will make a simple AI bot with whom i can play a game of Tic Tac Toe. \r\n","\r\n","\r\n","And it the bot will learn to play the game from the mistakes it would make in the process of doing so."]},{"cell_type":"markdown","metadata":{"id":"03iBd4jZ-Zep"},"source":["# Importing the important libraries and setting initial parameters"]},{"cell_type":"code","metadata":{"id":"8b68gTgH9Gbj","executionInfo":{"status":"ok","timestamp":1607493814818,"user_tz":-330,"elapsed":1679,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["import random\r\n","import json\r\n","\r\n","# gamma is the discount factor\r\n","gamma = 0.9 \r\n","\r\n","# alpha is the learning rate\r\n","alpha = 0.1"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Kupc7GS_A1_"},"source":["A function to find who won the game or has the match tie"]},{"cell_type":"code","metadata":{"id":"s-YxH5MC_PlN","executionInfo":{"status":"ok","timestamp":1607493821823,"user_tz":-330,"elapsed":1443,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def winner(state):\r\n","    for i in range(3):\r\n","        s = state[i][0] + state[i][1] + state[i][2]\r\n","        if (s==0):\r\n","            return 0\r\n","        elif (s==3):\r\n","            return 1\r\n","        \r\n","        s1 = state[0][i] + state[1][i] + state[2][i]\r\n","        if (s1==0):\r\n","            return 0\r\n","        elif (s1==3):\r\n","            return 1\r\n","        \r\n","    ld = state[0][0] + state[1][1] + state[2][2]\r\n","    rd = state[0][2] + state[1][1] + state[2][0]\r\n","    \r\n","    if(ld==0):\r\n","        return 0\r\n","    elif(ld==3):\r\n","        return 1\r\n","    \r\n","    if(rd==0):\r\n","        return 0\r\n","    elif(rd==3):\r\n","        return 1\r\n","    \r\n","    return -1\r\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7i9kvHMc_TZY"},"source":["A function to check whether game has ended or not"]},{"cell_type":"code","metadata":{"id":"wW35s39Q_Z19","executionInfo":{"status":"ok","timestamp":1607493828266,"user_tz":-330,"elapsed":1753,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def game_end(state):\r\n","    \r\n","    k = 0\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            if(state[i][j] == -10):\r\n","                k += 1\r\n","\r\n","    if(k==0):\r\n","        return True\r\n","    else:\r\n","        for i in range(3):\r\n","            s = state[i][0] + state[i][1] + state[i][2]\r\n","            if (s==0 or s==3):\r\n","                return True\r\n","            s1 = state[0][i] + state[1][i] + state[2][i]\r\n","            if (s1==0 or s1==3):\r\n","                return True\r\n","        ld = state[0][0] + state[1][1] + state[2][2]\r\n","        rd = state[0][2] + state[1][1] + state[2][0]\r\n","        \r\n","        if(ld==0 or ld==3):\r\n","            return True\r\n","        if(rd==0 or rd==3):\r\n","            return True\r\n","        return False\r\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kv2_U7bo_dE2"},"source":["A function to display the current state of the game after each move"]},{"cell_type":"code","metadata":{"id":"Dl8Iolg8_lZE","executionInfo":{"status":"ok","timestamp":1607493833024,"user_tz":-330,"elapsed":1595,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def display(grid):\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            if(grid[i][j]==1):\r\n","                print(\"X\", end = \" \")\r\n","            elif(grid[i][j]==0):\r\n","                print(\"O\", end = \" \")\r\n","            else:\r\n","                print(\"_\", end = \" \")\r\n","        print()\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaonJrB1_mgw"},"source":["An important function to check if the current state of the game is previously visited and is present in the value dictionary"]},{"cell_type":"code","metadata":{"id":"owNxlQuLAed9","executionInfo":{"status":"ok","timestamp":1607493837567,"user_tz":-330,"elapsed":1803,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["# grid is the game state\r\n","# value is the dictionary mapping the game state to a value\r\n","def is_present(grid, value):\r\n","    \r\n","    G0 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G1 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G2 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G3 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G4 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G5 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G6 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    G7 = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    \r\n","\r\n","    # In each step here json is used to convert the gris state which \r\n","    # is a 2d matrix to a mappable feature that could be mapped to a value\r\n","    G0 = json.dumps(list(map(list, grid)))\r\n","    \r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G1[i][j] = grid[i][2-j]\r\n","    G1 = json.dumps(list(map(list, G1)))\r\n","\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G2[i][j] = grid[2-i][j]\r\n","    G2 = json.dumps(list(map(list, G2)))\r\n","    \r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G3[i][j] = grid[2-i][2-j]\r\n","    G3 = json.dumps(list(map(list, G3)))\r\n","    \r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G4[i][j] = grid[j][i]\r\n","    G4 = json.dumps(list(map(list, G4)))\r\n","    \r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G5[i][j] = grid[j][2-i]\r\n","    G5 = json.dumps(list(map(list, G5)))\r\n","    \r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G6[i][j] = grid[2-j][i]\r\n","    G6 = json.dumps(list(map(list, G6)))\r\n","\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            G7[i][j] = grid[2-j][2-i]\r\n","    G7 = json.dumps(list(map(list, G7)))\r\n","    \r\n","    if(G0 in value):\r\n","        return (value[G0])\r\n","    elif(G1 in value):\r\n","        return (value[G1])\r\n","    elif(G2 in value):\r\n","        return (value[G2])\r\n","    elif(G3 in value):\r\n","        return (value[G3])\r\n","    elif(G4 in value):\r\n","        return (value[G4])\r\n","    elif(G5 in value):\r\n","        return (value[G5])\r\n","    elif(G6 in value):\r\n","        return (value[G6])\r\n","    elif(G7 in value):\r\n","        return (value[G7])\r\n","    else:\r\n","        value[G0] = 0\r\n","        return 0\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EHJaxHHfBJL8"},"source":["This is a very important function that makes the agent intelligent fast\r\n","and makes it learn the game more robustly.\r\n","In this function there is an opponent agent with preprogrammed strategies as follows:\r\n","\r\n","1: if the AI agent is going to win with his 2 marks present in one line, then it would block the last mark on the line.\r\n","\r\n","2: If this agent has 2 marks on one line , its policy will be to always do a mark on the last position in the line and thus win the game.\r\n","\r\n","\r\n","I will train my AI agent against this preprogrammed agent and make it learn the game more robustly."]},{"cell_type":"code","metadata":{"id":"XayabKUpCXpS","executionInfo":{"status":"ok","timestamp":1607493843602,"user_tz":-330,"elapsed":1589,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def opponent_policy(state, action):\r\n","    \r\n","    for i in range(3):\r\n","        p = state[i][0] + state[i][1] + state[i][2]\r\n","        if(p==-8):\r\n","            for j in range(3):\r\n","                if(state[i][j]==-10):\r\n","                    return ([i,j])\r\n","        p1 = state[0][i] + state[1][i] + state[2][i]\r\n","        if(p1==-8):\r\n","            for j in range(3):\r\n","                if(state[j][i]==-10):\r\n","                    return ([j,i])\r\n","        \r\n","    ld1 = state[0][0] + state[1][1] + state[2][2]\r\n","    rd1 = state[0][2] + state[1][1] + state[2][0]\r\n","    \r\n","    if(ld1==-8):\r\n","        for i in range(3):\r\n","            if(state[i][i]==-10):\r\n","                return ([i,i])\r\n","    \r\n","    if(rd1==-8):\r\n","        for i in range(3):\r\n","            if(state[i][2-i]==-10):\r\n","                return ([i,2-i])\r\n","    \r\n","    for i in range(3):\r\n","        p = state[i][0] + state[i][1] + state[i][2]\r\n","        if(p==-10):\r\n","            for j in range(3):\r\n","                if(state[i][j]==-10):\r\n","                    return ([i,j])\r\n","        p1 = state[0][i] + state[1][i] + state[2][i]\r\n","        if(p1==-10):\r\n","            for j in range(3):\r\n","                if(state[j][i]==-10):\r\n","                    return ([j,i])\r\n","        \r\n","    ld1 = state[0][0] + state[1][1] + state[2][2]\r\n","    rd1 = state[0][2] + state[1][1] + state[2][0]\r\n","    \r\n","    if(ld1==-10):\r\n","        for i in range(3):\r\n","            if(state[i][i]==-10):\r\n","                return ([i,i])\r\n","    \r\n","    if(rd1==-10):\r\n","        for i in range(3):\r\n","            if(state[i][2-i]==-10):\r\n","                return ([i,2-i])\r\n","    \r\n","    return (random.choice(action))\r\n"," "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3gguoFNCZ7W"},"source":["A funtion to play the episodes of the game in which the AI agent plays against the random choice agent , which is not a good player."]},{"cell_type":"code","metadata":{"id":"jNXeZKf6CxU-","executionInfo":{"status":"ok","timestamp":1607493849003,"user_tz":-330,"elapsed":1486,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def play_episode(s, episode):\r\n","\r\n","    # action is a list of actions available for the agent to perform\r\n","    # s is the action performed\r\n","    action = []\r\n","    grid = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]] # this is the empty grid state\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            action.append([i, j])\r\n","           \r\n","    action.remove(s)\r\n","    grid[s[0]][s[1]] = 1\r\n","    temp1 = list(map(list, grid))\r\n","    episode.append(temp1)\r\n","\r\n","    # play the episode until someone wins or match ties\r\n","    while(not(game_end(grid))):\r\n","        # agent performs a random move\r\n","        agent = random.choice(action)\r\n","        action.remove(agent)\r\n","        grid[agent[0]][agent[1]] = 0\r\n","        temp = list(map(list, grid))\r\n","        episode.append(temp)\r\n","        \r\n","        if((game_end(grid))):\r\n","            break\r\n","        # random move agent also performs a random move\r\n","        user = random.choice(action)\r\n","        action.remove(user)\r\n","        grid[user[0]][user[1]] = 1\r\n","        temp = list(map(list, grid))\r\n","        episode.append(temp)\r\n","        \r\n","        if((game_end(grid))):\r\n","            break\r\n","    "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSiDQ3GZDL7t"},"source":["Initializing the parameters"]},{"cell_type":"code","metadata":{"id":"VNUHgBlSD4jX","executionInfo":{"status":"ok","timestamp":1607493854176,"user_tz":-330,"elapsed":1522,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["value = {}\r\n","h = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","h1 = json.dumps(h)\r\n","value[h1] = 0"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6aV46dqWD7iJ"},"source":["Training the AI agent against the random policy agent for 10000 episodes"]},{"cell_type":"code","metadata":{"id":"9vydmMAeEF4y","executionInfo":{"status":"ok","timestamp":1607493890337,"user_tz":-330,"elapsed":33489,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["for ep in  range(10000):\r\n","    # I have kept the intial action only 4 because rest of the actions are\r\n","    # symmetrical with these actions\r\n","    in_act = [[0,0],[0,1],[1,0],[1,1]]\r\n","\r\n","    for ch in range(4):\r\n","        \r\n","        s = in_act[ch]\r\n","        # episode is a list which will consist of all the game states in a complete episode of the game\r\n","        episode = []\r\n","        play_episode(s, episode) \r\n","        end = episode[len(episode)-1]\r\n","        end_hash = json.dumps(end)\r\n","        if(winner(end)==0):\r\n","          # winning state has a large positive value of 5\r\n","            value[end_hash] = 5\r\n","        elif(winner(end)==1):\r\n","          # losing state has a large negative value of 5\r\n","            value[end_hash] = -5\r\n","        else:\r\n","          # match tie state has a intermediatevalue of positive 2\r\n","            value[end_hash] = 2\r\n","          \r\n","\r\n","        # performing the TD learning (temporal difference learning) on each episode\r\n","        for i in range(len(episode)-1):\r\n","            s = is_present(episode[i], value)\r\n","            ns = is_present(episode[i+1], value)\r\n","            S = json.dumps(episode[i])\r\n","            if(i==len(episode)-2 and value[json.dumps(episode[i+1])]==5):\r\n","                value[S] = s + alpha*(2 + gamma*ns - s)\r\n","            elif(i==len(episode)-2 and value[json.dumps(episode[i+1])]==-5):\r\n","                value[S] = s + alpha*(-2 + gamma*ns - s)\r\n","            elif(i==len(episode)-2 and value[json.dumps(episode[i+1])]==2):\r\n","                value[S] = s + alpha*(1 + gamma*ns - s)\r\n","            else:\r\n","                value[S] = s + alpha*(-1 + gamma*ns - s)\r\n","  "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccrRHIUXEHxr"},"source":["Making a copy of the values of the states the agent could find"]},{"cell_type":"code","metadata":{"id":"QzQhqZABEi1l","executionInfo":{"status":"ok","timestamp":1607493898938,"user_tz":-330,"elapsed":1418,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["value1 = value.copy()"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Krpy6mLcEjtP"},"source":["A function to make the AI agent play gainst the preprogrammed agent"]},{"cell_type":"code","metadata":{"id":"czqbV6nmEzHS","executionInfo":{"status":"ok","timestamp":1607493903579,"user_tz":-330,"elapsed":1640,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def play_intelligent_episode(s,episode):\r\n","    \r\n","    action = []\r\n","    grid = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            action.append([i, j])\r\n","           \r\n","    action.remove(s)\r\n","    grid[s[0]][s[1]] = 1\r\n","    temp1 = list(map(list, grid))\r\n","    episode.append(temp1)\r\n","    \r\n","    while(not(game_end(grid))):\r\n","        \r\n","        Max = -100.0\r\n","        \r\n","        # In this loop the agent always performs the best move it could make\r\n","        for act in action:\r\n","            Grid = list(map(list, grid))\r\n","            Grid[act[0]][act[1]] = 0\r\n","            val = is_present(Grid, value1)\r\n","            Max = max(Max, val)\r\n","            if(Max == val):\r\n","                agent = act\r\n","                \r\n","        action.remove(agent)\r\n","        grid[agent[0]][agent[1]] = 0\r\n","        temp = list(map(list, grid))\r\n","        episode.append(temp)\r\n","        \r\n","        \r\n","        \r\n","        if((game_end(grid))):\r\n","            break\r\n","        \r\n","        # user performs with the preprogrammed agent policy\r\n","        user = opponent_policy(grid, action)\r\n","        action.remove(user)\r\n","        grid[user[0]][user[1]] = 1\r\n","        temp = list(map(list, grid))\r\n","        episode.append(temp)\r\n","        \r\n","        \r\n","        \r\n","        if((game_end(grid))):\r\n","            break\r\n","        "],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZBNYr66E03I"},"source":["Setting the learning rate to a higher value so that it could learn more robustly"]},{"cell_type":"code","metadata":{"id":"wV_rv1ssFI__","executionInfo":{"status":"ok","timestamp":1607493909851,"user_tz":-330,"elapsed":2013,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["alpha = 0.18"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMPYPjjVFPW5"},"source":["Training the AI agent against preprogrammed agent for 1000 episodes"]},{"cell_type":"code","metadata":{"id":"v1aeqpjGFXaN","executionInfo":{"status":"ok","timestamp":1607493916099,"user_tz":-330,"elapsed":3371,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["for ep in  range(1000):\r\n","  \r\n","    in_act = [[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\r\n","    episode = []\r\n","    S = random.choice(in_act)\r\n","\r\n","    # playing with the preprogrammed agent\r\n","    play_intelligent_episode(S, episode) \r\n","    end = episode[len(episode)-1]\r\n","    end_hash = json.dumps(end)\r\n","    if(winner(end)==0):\r\n","        value1[end_hash] = 5\r\n","    elif(winner(end)==1):\r\n","        value1[end_hash] = -5\r\n","    else:\r\n","        value1[end_hash] = 2\r\n","\r\n","    # performing the TD learning step\r\n","    for i in range(len(episode)-1):\r\n","        s = is_present(episode[i], value1)\r\n","        ns = is_present(episode[i+1], value1)\r\n","        S = json.dumps(episode[i])\r\n","        if(i==len(episode)-2 and value1[json.dumps(episode[i+1])]==5):\r\n","            value1[S] = s + alpha*(2 + gamma*ns - s)\r\n","        elif(i==len(episode)-2 and value1[json.dumps(episode[i+1])]==-5):\r\n","            value1[S] = s + alpha*(-2 + gamma*ns - s)\r\n","        elif(i==len(episode)-2 and value1[json.dumps(episode[i+1])]==2):\r\n","            value1[S] = s + alpha*(1 + gamma*ns - s)\r\n","        else:\r\n","            value1[S] = s + alpha*(-1 + gamma*ns - s)\r\n","   "],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hRr3tDF1FazG"},"source":["A function so that we could play with the agent after it gets trained"]},{"cell_type":"code","metadata":{"id":"D11re9UJFnUG","executionInfo":{"status":"ok","timestamp":1607493923722,"user_tz":-330,"elapsed":1656,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def play_userFirst(s, value, episode):\r\n","    \r\n","    action = []\r\n","    grid = [[-10, -10, -10], [-10, -10, -10], [-10, -10, -10]]\r\n","    for i in range(3):\r\n","        for j in range(3):\r\n","            action.append([i, j])\r\n","           \r\n","    action.remove(s)\r\n","    grid[s[0]][s[1]] = 1\r\n","    temp1 = list(map(list, grid))\r\n","    episode.append(temp1)\r\n","    print(\"your move :-\")\r\n","    display(grid)\r\n","    print()\r\n","    while(not(game_end(grid))):\r\n","        \r\n","        Max = -100.0\r\n","        \r\n","        for act in action:\r\n","            Grid = list(map(list, grid))\r\n","            Grid[act[0]][act[1]] = 0\r\n","            val = is_present(Grid, value)\r\n","            Max = max(Max, val)\r\n","            if(Max == val):\r\n","                agent = act\r\n","                \r\n","        action.remove(agent)\r\n","        grid[agent[0]][agent[1]] = 0\r\n","        temp = list(map(list, grid))\r\n","        episode.append(temp)\r\n","        \r\n","        print(\"agent's move :-\")\r\n","        display(grid)\r\n","        print()\r\n","        \r\n","        if((game_end(grid))):\r\n","            break\r\n","        \r\n","        print(\"enter your next move\")\r\n","        user = [int(x) for x in input().split(\",\")]\r\n","        action.remove(user)\r\n","        grid[user[0]][user[1]] = 1\r\n","        temp = list(map(list, grid))\r\n","        episode.append(temp)\r\n","        \r\n","        print(\"your move :-\")\r\n","        display(grid)\r\n","        print()\r\n","        \r\n","        if((game_end(grid))):\r\n","            break\r\n","        \r\n","    if(winner(grid)==1):\r\n","        print(\"You Win\")\r\n","    elif(winner(grid)==0):\r\n","        print(\"I Win\")\r\n","    else:\r\n","        print(\"Mtch Tie\")\r\n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CgyWAbqEFo5X"},"source":["Two important functions such that while playing with me the agent can learn from us too"]},{"cell_type":"code","metadata":{"id":"nYjiZksYGEuD","executionInfo":{"status":"ok","timestamp":1607493937700,"user_tz":-330,"elapsed":1538,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}}},"source":["def start_user1():\r\n","    alp = 0.3\r\n","    print(\"start the game\")\r\n","    print(\"format for input: \")\r\n","    print(\"position for your mark: n,n\")\r\n","    print(\"for example:\")\r\n","    print(\"1,1\")\r\n","    print(\"above input is for center mark\")\r\n","    \r\n","    S = [int(x) for x in input().split(\",\")]\r\n","    \r\n","    episode = []\r\n","    play_userFirst(S, value1, episode)\r\n","    end = episode[len(episode)-1]\r\n","    end_hash = json.dumps(end)\r\n","    if(winner(end)==0):\r\n","        value1[end_hash] = 5\r\n","    elif(winner(end)==1):\r\n","        value1[end_hash] = -5\r\n","    else:\r\n","        value1[end_hash] = 2\r\n","    for i in range(len(episode)-1):\r\n","        s = is_present(episode[i], value1)\r\n","        ns = is_present(episode[i+1], value1)\r\n","        S = json.dumps(episode[i])\r\n","        if(i==len(episode)-2 and value1[json.dumps(episode[i+1])]==5):\r\n","            value1[S] = s + alp*(2 + gamma*ns - s)\r\n","        elif(i==len(episode)-2 and value1[json.dumps(episode[i+1])]==-5):\r\n","            value1[S] = s + alp*(-2 + gamma*ns - s)\r\n","        elif(i==len(episode)-2 and value1[json.dumps(episode[i+1])]==2):\r\n","            value1[S] = s + alp*(1 + gamma*ns - s)\r\n","        else:\r\n","            value1[S] = s + alp*(-1 + gamma*ns - s)\r\n","\r\n","       \r\n","\r\n","\r\n","\r\n","def start_game():\r\n","        \r\n","    t1 = 1\r\n","    while(t1 is not 0):\r\n","        start_user1()\r\n","        print(\"do u want to play more 1 for yes 0 for no\")\r\n","        t1 = int(input())\r\n","        if(t1==0):\r\n","          print(\"It was nice to play with you. I learned few more things from you\")\r\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"diRrqcdeGG8C"},"source":["# Start the Game"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZTkkZcwGKGV","executionInfo":{"status":"ok","timestamp":1607494186662,"user_tz":-330,"elapsed":172098,"user":{"displayName":"karan verma","photoUrl":"","userId":"15923302943039472383"}},"outputId":"8b2ca954-163c-41f8-e09c-1d88b0478a50"},"source":["start_game()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["start the game\n","1,1\n","your move :-\n","_ _ _ \n","_ X _ \n","_ _ _ \n","\n","agent's move :-\n","_ O _ \n","_ X _ \n","_ _ _ \n","\n","enter your next move\n","0,0\n","your move :-\n","X O _ \n","_ X _ \n","_ _ _ \n","\n","agent's move :-\n","X O _ \n","_ X _ \n","_ _ O \n","\n","enter your next move\n","2,0\n","your move :-\n","X O _ \n","_ X _ \n","X _ O \n","\n","agent's move :-\n","X O _ \n","_ X O \n","X _ O \n","\n","enter your next move\n","1,0\n","your move :-\n","X O _ \n","X X O \n","X _ O \n","\n","You Win\n","do u want to play more 1 for yes 0 fro no\n","1\n","start the game\n","1,1\n","your move :-\n","_ _ _ \n","_ X _ \n","_ _ _ \n","\n","agent's move :-\n","_ _ O \n","_ X _ \n","_ _ _ \n","\n","enter your next move\n","2,2\n","your move :-\n","_ _ O \n","_ X _ \n","_ _ X \n","\n","agent's move :-\n","O _ O \n","_ X _ \n","_ _ X \n","\n","enter your next move\n","0,1\n","your move :-\n","O X O \n","_ X _ \n","_ _ X \n","\n","agent's move :-\n","O X O \n","_ X _ \n","_ O X \n","\n","enter your next move\n","1,2\n","your move :-\n","O X O \n","_ X X \n","_ O X \n","\n","agent's move :-\n","O X O \n","O X X \n","_ O X \n","\n","enter your next move\n","2,0\n","your move :-\n","O X O \n","O X X \n","X O X \n","\n","Mtch Tie\n","do u want to play more 1 for yes 0 fro no\n","1\n","start the game\n","1,1\n","your move :-\n","_ _ _ \n","_ X _ \n","_ _ _ \n","\n","agent's move :-\n","_ _ O \n","_ X _ \n","_ _ _ \n","\n","enter your next move\n","2,0\n","your move :-\n","_ _ O \n","_ X _ \n","X _ _ \n","\n","agent's move :-\n","_ _ O \n","_ X _ \n","X _ O \n","\n","enter your next move\n","1,2\n","your move :-\n","_ _ O \n","_ X X \n","X _ O \n","\n","agent's move :-\n","_ _ O \n","O X X \n","X _ O \n","\n","enter your next move\n","2,1\n","your move :-\n","_ _ O \n","O X X \n","X X O \n","\n","agent's move :-\n","_ O O \n","O X X \n","X X O \n","\n","enter your next move\n","0,0\n","your move :-\n","X O O \n","O X X \n","X X O \n","\n","Mtch Tie\n","do u want to play more 1 for yes 0 fro no\n","1\n","start the game\n","1,1\n","your move :-\n","_ _ _ \n","_ X _ \n","_ _ _ \n","\n","agent's move :-\n","_ _ O \n","_ X _ \n","_ _ _ \n","\n","enter your next move\n","1,0\n","your move :-\n","_ _ O \n","X X _ \n","_ _ _ \n","\n","agent's move :-\n","_ _ O \n","X X O \n","_ _ _ \n","\n","enter your next move\n","2,2\n","your move :-\n","_ _ O \n","X X O \n","_ _ X \n","\n","agent's move :-\n","O _ O \n","X X O \n","_ _ X \n","\n","enter your next move\n","2,0\n","your move :-\n","O _ O \n","X X O \n","X _ X \n","\n","agent's move :-\n","O O O \n","X X O \n","X _ X \n","\n","I Win\n","do u want to play more 1 for yes 0 fro no\n","0\n"],"name":"stdout"}]}]}